{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97db3596",
   "metadata": {},
   "source": [
    "#### PySpark Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42206191",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#docker command\n",
    "docker run -it \\\n",
    "  -v ./work:/home/jovyan/work \\\n",
    "  --user root \\\n",
    "  -e CHOWN_HOME=yes \\\n",
    "  -e CHOWN_HOME_OPTS='-R' \\\n",
    "  -p 8888:8888 \\\n",
    "  jupyter/pyspark-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ba77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "556770b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9a169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.appName(\"SparkPlayground\").getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c34b37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1000\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(range(1000))\n",
    "print(\"Count: \", rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6deba596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from parallelize    \n",
    "dataList = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000), (\"Python\", 30000), (\"C++\", 100000), (\"Java\", 50000)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "501274b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD operations\n",
      "Count:  6\n",
      "First:  ('Java', 20000)\n",
      "Take:  [('Java', 20000), ('Python', 100000)]\n",
      "Collect:  [('Java', 20000), ('Python', 100000), ('Scala', 3000), ('Python', 30000), ('C++', 100000), ('Java', 50000)]\n"
     ]
    }
   ],
   "source": [
    "# Examples of RDD operations\n",
    "print(\"RDD operations\")\n",
    "print(\"Count: \", rdd.count())\n",
    "print(\"First: \", rdd.first())\n",
    "print(\"Take: \", rdd.take(2))\n",
    "print(\"Collect: \", rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "921d39d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD transformations\n",
      "Map:  [('Java', 40000), ('Python', 200000), ('Scala', 6000), ('Python', 60000), ('C++', 200000), ('Java', 100000)]\n",
      "Filter:  [('Python', 100000), ('Python', 30000)]\n",
      "FlatMap:  ['Java', 40000, 'Python', 200000, 'Scala', 6000, 'Python', 60000, 'C++', 200000, 'Java', 100000]\n",
      "GroupByKey:  [('Scala', [3000]), ('C++', [100000]), ('Java', [20000, 50000]), ('Python', [100000, 30000])]\n",
      "ReduceByKey:  [('Scala', 3000), ('C++', 100000), ('Java', 70000), ('Python', 130000)]\n",
      "SortByKey:  [('C++', 100000), ('Java', 20000), ('Java', 50000), ('Python', 100000), ('Python', 30000), ('Scala', 3000)]\n"
     ]
    }
   ],
   "source": [
    "# Examples of RDD transformations\n",
    "print(\"RDD transformations\")\n",
    "print(\"Map: \", rdd.map(lambda x: (x[0], x[1] * 2)).collect())\n",
    "print(\"Filter: \", rdd.filter(lambda x: \"P\" in x[0]).collect())\n",
    "print(\"FlatMap: \", rdd.flatMap(lambda x: (x[0], x[1] * 2)).collect())\n",
    "print(\"GroupByKey: \", rdd.groupByKey().mapValues(list).collect())\n",
    "print(\"ReduceByKey: \", rdd.reduceByKey(lambda x, y: x + y).collect())\n",
    "print(\"SortByKey: \", rdd.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4387f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframe\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc31adb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show first elements:\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show first elements\n",
    "print(\"Show first elements:\")\n",
    "print(df.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82707075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF operations\n",
      "Count:  5\n",
      "First:  Row(firstname='James', middlename='', lastname='Smith', dob='1991-04-01', gender='M', salary=3000)\n",
      "Take:  [Row(firstname='James', middlename='', lastname='Smith', dob='1991-04-01', gender='M', salary=3000), Row(firstname='Michael', middlename='Rose', lastname='', dob='2000-05-19', gender='M', salary=4000)]\n",
      "Collect:  [Row(firstname='James', middlename='', lastname='Smith', dob='1991-04-01', gender='M', salary=3000), Row(firstname='Michael', middlename='Rose', lastname='', dob='2000-05-19', gender='M', salary=4000), Row(firstname='Robert', middlename='', lastname='Williams', dob='1978-09-05', gender='M', salary=4000), Row(firstname='Maria', middlename='Anne', lastname='Jones', dob='1967-12-01', gender='F', salary=4000), Row(firstname='Jen', middlename='Mary', lastname='Brown', dob='1980-02-17', gender='F', salary=-1)]\n"
     ]
    }
   ],
   "source": [
    "# DF operations\n",
    "print(\"DF operations\")\n",
    "print(\"Count: \", df.count())\n",
    "print(\"First: \", df.first())\n",
    "print(\"Take: \", df.take(2))\n",
    "print(\"Collect: \", df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13601002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL simple select\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "336008fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     M|       3|\n",
      "|     F|       2|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL M/F stats\n",
    "groupDF = spark.sql(\"SELECT gender, count(*) from people group by gender\")\n",
    "groupDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a733699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading from kafka topic\n",
    "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"pyspark\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3db41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Writing to kafka topic the same message with its index\n",
    "# df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#     .option(\"topic\", \"pyspark-write\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/kafka\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
